n = 25; % number of samples
d = 10; % input space dimensionality
k = 25; % embedding space dimensionality


% Aim is to approximate a linear kernel
% X = randn(d,n);

X = ...
   [0.7832    1.2722   -0.7392    1.0125   -0.5166   -1.7902   -0.0405   -0.1380    3.6953    1.3312    0.4674    1.0917    0.3598    0.5825   -0.3234    0.0237    0.3502    1.1720    1.2841    0.0606    0.2739   -0.0766   -0.8944   -0.2980   -0.2120;
   -0.3917    0.0390   -0.9696    0.7759    0.1503   -0.3946    1.4460    1.7904    0.7039   -0.2959   -0.7484    0.9258    1.1041   -0.3075   -1.2707   -0.0757   -1.2588   -1.1348    0.7606   -1.1588   -1.7229    0.9328   -0.8394    0.0487   -0.2951;
    0.4676   -0.6220    0.6016   -1.2425   -0.4559   -0.5176   -0.5459    0.2487    0.3824   -0.3401    0.8921   -1.0241   -0.1688   -0.2731   -0.5397    0.4708    0.8971   -0.5732    0.2780    0.4139   -0.9349   -2.2040    0.8927   -0.1725    0.3755;
    0.4539   -2.0851   -0.5522    0.0114   -0.7019    1.3585    0.5731    0.6748    0.2072    1.3283    1.2124    0.5269   -0.2416   -0.7360    0.9278    0.1791    0.4946    0.7702   -0.4865    0.6145    2.0150    3.1555    1.3209    0.1458   -0.6439;
    0.5132   -0.2693   -1.6352    0.9116   -1.0928   -0.8719    0.6756   -0.0058    1.8590   -0.2743    0.7361   -0.3908    1.0104   -1.2331    0.1614   -1.3100    1.0315   -1.2622    0.4752   -0.1804    1.5850   -0.1895   -0.5894   -1.3855   -0.3294;
    0.0952    0.2019   -1.2017   -1.6508    0.7478   -0.4882    1.0743   -0.2862    0.9590   -1.2744    3.3222    1.1735   -2.0597   -0.4550   -0.9825    1.0117    0.3584   -1.2106    0.5364   -1.4812   -0.8599    0.2943    1.1507   -1.3799    1.2049;
   -0.9261   -0.2313   -1.8657   -0.5591   -0.2977    0.6166    1.2716    0.8845    1.3604    0.3475    0.0221   -0.4749   -0.7684    0.0961   -0.8901   -0.7973   -0.3927    1.5487    0.8204   -0.8597   -0.2141    1.9429   -1.3695    1.4851   -0.9241;
   -0.4920    1.0148    0.7211    0.3302    0.2317   -2.3643   -0.0340    1.3523    0.3514    0.3626    1.4019   -0.6949    0.1966   -0.7096   -1.6821    0.7778   -2.4533    0.9329   -0.5156   -1.7617    1.4499    0.5327   -1.0023    0.8587    0.4608;
    0.9114    0.7777    1.2953    0.0607   -0.8540    1.9378   -0.5431   -0.8159   -1.9033    1.2149   -1.3264    0.3271   -1.5510   -0.6242    1.5347   -1.0559   -0.5543   -0.9658   -1.2648    0.6122    1.5263    0.3107    0.1099   -1.1802   -1.4937;
   -0.1146   -0.5232    0.3694    0.0146    1.8325    0.6620    0.5606   -1.7088    1.3448    0.1951   -1.5425   -0.7235   -0.1529    1.3353   -1.5948   -1.3390    0.1149   -0.7745   -0.4364   -0.6045    1.9477    1.7257    0.3311    1.2968   -0.8311];

K_actual = X' * X;


% p = 1.5 + rand(); % Fixed value p > 1
p = 2;
fh_p = @(p,n) 1./p.^(n+1);
tmp = fh_p(p, 0:100);
tmp = tmp / sum(tmp); % now we have a discrete probability distribution whose sum is 1.
tabulate(discretesample(tmp, 100) - 1) % sanity check, the first few elements should have the highest count!


% Maybe use this for higher order polynomials as well.
% Drawback is that we have to calculate the derivative for each k.
% syms f(x)
% f(x) = x;
% df = diff(f,x,N)
% a_N = df(0);


Z = zeros(k,n);
for i = 1 : k
  N = discretesample(tmp, 1) - 1; % -1 because we want a non-negative integer, which allows for 0. discretesample's first index is 1, ... so we simply subtract by 1.
  if N == 1
    a_N = 1; % for linear kernels
  else
    a_N = 0;
  end
  % N = 1;
  % a_N = 1;
  W = sign(randn(d, N)); % choosing N vectors w_1, ..., w_N \in {-1,+1}^d
  Z_i = sqrt(a_N * p ^ (N + 1)) * prod(W' * X, 1);
  assert(isequal(size(Z_i), [1,n]));
  Z(i,:) = Z_i;
end
% Z = 1/sqrt(k) * Z * 1/sqrt(3);
Z = 1/sqrt(k) * Z;
K_approx = Z' * Z;


K_diff = (K_approx - K_actual).^2;
figure,
subplot(1,2,1),
imshow(K_actual),
title(sprintf('|K actual - K approx|_F = %.3f', sqrt(sum(K_diff(:)))));
subplot(1,2,2),
imshow(K_approx),
title(sprintf('|K actual - K approx|_F = %.3f', norm(K_actual - K_approx, 'fro')));

K_actual(1:5,1:5)
K_approx(1:5,1:5)

K_approx(1:5,1:5) ./ K_actual(1:5,1:5)

